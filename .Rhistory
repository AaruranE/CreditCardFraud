rnorm(10)
plot(x,y)
x <- runif(N, xMin, xMax)
N <- 100
errorWidth <- 5
x <- runif(N, xMin, xMax)
error <- rnorm(N)*errorWidth
y <- 3*x + error
xMax <- 100
xMin <- 0
N <- 100
errorWidth <- 5
x <- runif(N, xMin, xMax)
error <- rnorm(N)*errorWidth
y <- 3*x + error
plot(x,y)
install.packages('dplyr')
installed.packages()
?read.table()
Auto = read.table("Auto.data")
download.packages("ISLR")
download.packages("ISLR")
download.packages("ISLR")
download.packages("ISLR",pwd)
pwd
data(college)
college = read.csv("College", header = TRUE)
#college = read.csv("College", header = TRUE)
#college = read.csv("College", header = TRUE)
library(islr)
installed.packages('ISLR')
install.packages("islr")
install.packages("ISLR")
library(islr)
library(ISLR)
college = read.table("College.data")
data(College)
college = read.table("College.csv")
college = read.table("College.csv", header = TRUE)
library(ISLR)
data(College)
college = read.table("College.csv", header = TRUE)
fix(college)
fix(College)
rownames(college) = college[,1]
rownames(College) = College[,1]
install.packages("shiny")
install.packages("shiny")
install.packages("shiny")
library(MASS)
library(ISLR)
install.packages("ISLR")
install.packages("ISLR")
library(ISLR)
version()
R.version()
R.version
install.packages("ISLR")
install.packages("ISLR")
library(ISLR)
fix(Boston)
?Boston
install.packages("MASS")
library(MASS)
library(ISLR)
fix(Boston)
?Boston
names(Boston)
?fix
Boston()
Boston
rm(Boston())
rm(Boston)
?Boston
Boston
names(Boston)
lm.fit=lm(medv~lstat)
names(Boston)
attach(Boston)
?attach
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
coef(lm.fit)
confint(lm.fit)
interval = "confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)),
interval = "confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)),interval = "prediction")
plot(lstat, medv)
abline(lm.fit)
abline(lm.fit, lwd=3)
plot(lstat,medv,pch="+")
plot(lstat,medv,pch="+", col="red")
plot(1:20,1:20,pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
lm.fit = lm(medv~lstat + age)
summary(lm.fit)
lm.fit = lm(medv~.)
lm.fit = lm(medv~., data = Boston)
summary(lm.fit)
library(car)
install.packages("car")
library(car)
vif(lm.fit)
?vif
lm.fit1 = update(lm.fit, ~.-age)
summary(lm(medv~lstat+age))
summary(lm(medv~lstat+age,data = Boston))
lm.fit2 = lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
lm.fit = lm(medv~lstat)
anova(lm.fit,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
summary(lm(medv~log(rm),data=Boston))
summary(lm.fit5)
setwd("C:/Users/Aaruran/Documents/GitHub/Machine learning Self-study/ccfraud/")
knitr::opts_chunk$set(echo = FALSE)
df <- read.csv("creditcardfraud/creditcard.csv")
attach(df)
df$Class <- as.factor(df$Class)
library(ggplot2)
library(dplyr)
library(reshape2)
library(tidyr)
df.class0 <- dplyr::filter(df, Class==0)
df.class1 <- dplyr::filter(df, Class==1)
proportion <- as.numeric(length(df.class1[,1])/length(df.class0[,1]))
training.size0 <- floor(0.7*nrow(df.class0))
training.size1 <- floor(0.7*nrow(df.class1))
set.seed(115)
train.index.0 <- sample(seq_len(nrow(df.class0)), size=training.size0)
train.index.1 <- sample(seq_len(nrow(df.class1)), size=training.size1)
train <- merge(df.class0[train.index.0,], df.class1[train.index.1,])
train <- full_join(df.class0[train.index.0,], df.class1[train.index.1,])
test <-  full_join(df.class0[-train.index.0,], df.class1[-train.index.1,])
?merge
means <- colMeans(train)
train.classes <- train$class
train$class <- NULL
means <- colMeans(train)
means <- colMeans(as.matrix(train))
train.classes <- train$Class
train$Class <- NULL
means <- colMeans(as.matrix(train))
?ncol
train.Adjusted <- train
train.Adjusted[row, col] <- train.Adjusted[row,col] - means[1,col]
for(col in 1:ncol(train))
{
for(row in 1:nrow(train))
{
train.Adjusted[row, col] <- train.Adjusted[row,col] - means[1,col]
}
}
for(col in 1:ncol(train))
{
for(row in 1:nrow(train))
{
train.Adjusted[row, col] <- train.Adjusted[row,col] - means[col]
}
}
View(train.Adjusted)
?scale
train.Adjusted <- scale(train, center=TRUE)
cov.train <- cov(train.Adjusted)
?eigen
pca <- eigen(cov.train, symmetric=TRUE,only.values=FALSE)
```{eigen covariance,echo=TRUE, message=TRUE, cache=TRUE}
pca$v
pca$vectors
print(pca$values)
plot(pca$values)
pca$values > 0.5
pca$values[pca$values > 0.5]
View(pca$vectors)
principalValues <- pca$values[pca$values > 0.5]
featureVector <- pca$vectors[pca$values > 0.5]
featureVector <- as.matrix(pca$vectors[pca$values > 0.5], ncol=28)
featureVector <- pca$vectors[,pca$values > 0.5]
t(featureVector[,1])*featureVector[,1]
sum(t(featureVector[,1])*featureVector[,1])
train <- df.class0[train.index.0,]
test <-  df.class0[-train.index.0,]
train.classes <- train$Class
train$Class <- NULL
train.Adjusted <- scale(train, center=TRUE)
cov.train <- cov(train.Adjusted)
pca <- eigen(cov.train, symmetric=TRUE,only.values=FALSE)
print(pca$values)
plot(pca$values)
principalValues <- pca$values[pca$values > 0.5]
featureVector <- pca$vectors[,pca$values > 0.5]
cor.train <- cor(train.Adjusted)
pca <- eigen(cor.train, symmetric=TRUE,only.values=FALSE)
rm(cov.train)
print(pca$values)
plot(pca$values)
pca <- eigen(cor.train, symmetric=TRUE,only.values=FALSE)
print(pca$values)
plot(pca$values)
principalValues <- pca$values[pca$values > 0.5]
featureVector <- pca$vectors[,pca$values > 0.5]
principalValues <- pca$values[pca$values > 0.2]
featureVector <- pca$vectors[,pca$values > 0.2]
?prcomp
test.dim_reduced <- train[,-29:31]
test.dim_reduced <- train[,1:28]
featureVector*t(test.dim_reduced)
featureVector*t(test.dim_reduced)
cc.pca <- prccomp(train, center=TRUE, scale=TRUE)
library(stats)
library(stats)
cc.pca <- prccomp(train, center=TRUE, scale=TRUE)
install.packages("Stats")
install.packages("stats")
install.packages("stats")
cc.pca <- prccomp(train, center=TRUE, scale=TRUE)
cc.pca <- prcomp(train, center=TRUE, scale=TRUE)
print(cc.pca)
plot(cc.pca, method="l")
summary(cc.pca)
train.noClass <- train[,1:29]
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
print(cc.pca)
plot(cc.pca, method="l")
summary(cc.pca)
train.noClass <- train[,1:30]
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
print(cc.pca)
plot(cc.pca, method="l")
summary(cc.pca)
predict(cc.pca, newdata=test[,1:30])
predicted <- predict(cc.pca, newdata=test[,1:30])
train.noClass <- train
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
plot(cc.pca, method="l")
summary(cc.pca)
predicted <- predict(cc.pca, newdata=test[,1:30])
View(predicted)
?predict
cc.pca$x
dimension(cc.pca$x)
dim(cc.pca$x)
?prcomp
cc.pca
cc.pca$center
cc.pca$scale
cc.pca$sdev
cc.pca$center
train.noClass <- train[,2:30]
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
plot(cc.pca, method="l")
plot(cc.pca)
plot(cc.pca)
summary(cc.pca)
alpha <- proportion
?zeros
?zero
?vector
predicted.Class <- vector(mode="numeric", length=length(predicted))
?foreach
pca
cc.pca$x
predicted <- predict(cc.pca, newdata=test[,2:30])
alpha <- proportion
predicted.Class <- vector(mode="numeric", length=length(predicted))
summary(cc.pca)
train.noClass <- train
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
plot(cc.pca)
predicted <- predict(cc.pca, newdata=test)
predicted
View(predicted)
summary(predicted)
setwd("C:/Users/Aaruran/Documents/GitHub/Machine learning Self-study/ccfraud/")
df <- read.csv("creditcardfraud/creditcard.csv")
df <- read.csv("creditcardfraud/creditcard.csv")
attach(df)
df$Class <- as.factor(df$Class)
summary(df)
library(ggplot2)
library(dplyr)
library(reshape2)
library(tidyr)
df %>% select(-Time, -Class) %>%reshape2::melt() %>% ggplot(aes(x=value)) + geom_histogram() + facet_wrap(~variable,scales = "free")
?melt
cor.df <- cor(df)
cor.df <- cor(df[-31])
library(corrgram)
corrgram(cor.df,type='corr',order=TRUE, panel=panel.shade)
library(stats)
train.noClass <- train
df.class0 <- dplyr::filter(df, Class==0)
df.class1 <- dplyr::filter(df, Class==1)
proportion <- as.numeric(length(df.class1[,1])/length(df.class0[,1]))
training.size0 <- floor(0.7*nrow(df.class0))
set.seed(115)
train.index.0 <- sample(seq_len(nrow(df.class0)), size=training.size0)
train <- df.class0[train.index.0,]
test <-  df.class0[-train.index.0,]
train.noClass <- train
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
train <- df.class0[train.index.0,-31]
train.noClass <- train
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
cc.pca <- prcomp(train.noClass, center=TRUE, scale=TRUE)
plot(cc.pca)
summary(cc.pca)
predicted <- predict(cc.pca, newdata=test[,-31])
predicted.Class <- vector(mode="numeric", length=length(predicted))
plot(predicted)
qplot(predicted)
View(predicted)
ggplot(predicted, aes(x=PC1, y=PC2))
predicted %>% as.data.frame %>% ggplot( aes(x=PC1, y=PC2))+ geom_point()
rm(list = c(train, test ))
rm(train)
rm(test)
rm(train.noClass)
rm(train.index.0)
rm(train.index.1)
```{r clear, echo=FALSE, message=FALSE}
training.size <- 0.75*length(df)
train.nn1.index <- sample(seq_len(nrow(df)), size=training.size)
train.knn.index <- sample(seq_len(nrow(df)), size=training.size)
rm(train.nn1.index)
training.set <- df[train.knn.index,]
test.set <- df[-train.knn.index]
knn1 <- knn(training.set, test, Class, k=1, prob=TRUE, use.all  =TRUE)
library(class)
knn1 <- knn(training.set, test, Class, k=1, prob=TRUE, use.all  =TRUE)
knn1 <- knn(training.set, test.set, Class, k=1, prob=TRUE, use.all  =TRUE)
knn1 <- knn(training.set[,-31], test.set[,-31], training.set[,31], k=1, prob=TRUE, use.all  =TRUE)
knn1
training.size <- 0.75*nrow(df)
training.size <- floor(0.75*nrow(df))
train.knn.index <- sample(seq_len(nrow(df)), size=training.size)
training.set <- df[train.knn.index,]
test.set <- df[-train.knn.index]
knn1 <- knn(training.set[,-31], test.set[,-31], training.set[,31], k=1, prob=TRUE, use.all  =TRUE)
training.set <- df[train.knn.index,]
test.set <- df[-train.knn.index,]
knn1 <- knn(training.set[,-31], test.set[,-31], training.set[,31], k=1, prob=TRUE, use.all  =TRUE)
knn1
install.packages("PerfMeas")
library(PerfMeas)
install.packages("PerfMeas")
install.packages(c("limma", "graph", "RBGL"))
?installr
??installr
install.R()
installr::install.R()
install.packages("PerfMeas")
install.packages("PerfMeas",dependencies = TRUE)
source("https://bioconductor.org/biocLite.R")
biocLite("limma")
biocLite("graph")
biocLite("RBGL")
library(PerfMeas)
install.packages("PerMeas")
knn1 <- knn(training.set[,-31], test.set[,-31], training.set[,31], k=1, prob=FALSE, use.all  =TRUE)
truePositives < sum(as.numeric(knn1) & as.numeric(test.set$Class))
truePositives <- sum(as.numeric(knn1) & as.numeric(test.set$Class))
predictedPositives <- sum(knn1)
predictedPositives <- sum(as.numeric(knn1))
actualPositives <- sum(as.numeric(test.set$Class))
precision <- truePositives / predictedPositives
recall <- truePositives / actualPositives
Fmeasure <- 2 * precision * recall / (precision + recall)
print(Fmeasure)
library(FNN)
install.packages("FNN")
library(FNN)
knn1 <- FNN::knn(training.set[,c(-1,-31)], test.set[,c(-1,-31)], training.set[,31], k=1, prob=FALSE, use.all  =TRUE,algorithm = "kd_tree")
knn1 <- FNN::knn(training.set[,c(-1,-31)], test.set[,c(-1,-31)], training.set[,31], k=1, prob=FALSE, algorithm = "kd_tree")
?F
knnResults <- data.frame(k, Fscore)
View(df)
knnResults <- data.frame(k = integer(maxK), Fscore = numeric(maxK))
maxK <- 10
knnResults <- data.frame(k = integer(maxK), Fscore = numeric(maxK))
knnResults[1]
knnResults[1,]
knnResults$k
knnResults$k[1]
knnResults <- data.frame(k = seq(1,maxK), Fscore = numeric(maxK))
knnResults
knnResults$Fscore[1]
knnResults$Fscore[1] <- 3
knnResults
for(i in seq(1,maxk))
{
i.nn <- FNN::knn(train =training.set[,c(-1, -31)], test = test.set[,c(-1,-31)],
cl = training.set$Class, k = i, prob = FALSE, algorithm = "kd_tree")
truePositives <- sum(as.numeric(i.nn) & as.numeric(test.set$Class))
predictedPositives <- sum(as.numeric(i.nn))
actualPositives <- sum(as.numeric(test.set$Class))
i.precision <- truePositives / predictedPositives
i.recall <- truePositives / actualPositives
i.f1 <- 2*precision*recall / (precision + recall)
knnResults$Fscore[i] <- i.f1
}
for(i in seq(1,maxK))
{
i.nn <- FNN::knn(train =training.set[,c(-1, -31)], test = test.set[,c(-1,-31)],
cl = training.set$Class, k = i, prob = FALSE, algorithm = "kd_tree")
truePositives <- sum(as.numeric(i.nn) & as.numeric(test.set$Class))
predictedPositives <- sum(as.numeric(i.nn))
actualPositives <- sum(as.numeric(test.set$Class))
i.precision <- truePositives / predictedPositives
i.recall <- truePositives / actualPositives
i.f1 <- 2*precision*recall / (precision + recall)
knnResults$Fscore[i] <- i.f1
}
ggplot(knnResults, aes(x=k, y=Fscore)) + geom_point() + geom_line()
library("ggplot2", lib.loc="~/R/win-library/3.3")
library(ggplot2)
install.packages("ggplot2")
ggplot(knnResults, aes(x=k, y=Fscore)) + geom_point() + geom_line()
library(ggplot2)
install.packages("tidyr")
ggplot(knnResults, aes(x=k, y=Fscore)) + geom_point() + geom_line()
knnResults
seq(1,maxK)
plot(knnResults)
ggplotplot(knnResults, aes(x=k, y=Fscore)) + geom_point
ggplot(knnResults, aes(x=k, y=Fscore)) + geom_point
